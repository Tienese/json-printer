# Grammar Analysis Algorithm - Technical Specification

> **Version:** 3.0 (LSP-Style Adaptive Analysis)
> **Date:** 2025-12-25
> **Status:** Planning Complete - Ready for Implementation

---

## For Implementers (Opus 4.5)

### Execution Order (Follow This Exactly)

```
PHASE 0A: AUDIT
â”œâ”€â”€ Read: GRAMMAR_COACH_PHASE_0_AUDIT.md
â”œâ”€â”€ Scan entire codebase
â”œâ”€â”€ Document current state
â””â”€â”€ Output: Audit Report

PHASE 0B: GAP ANALYSIS
â”œâ”€â”€ Read: GRAMMAR_COACH_GAP_ANALYSIS.md
â”œâ”€â”€ Read: GRAMMAR_ALGORITHM_SPEC.md (this file)
â”œâ”€â”€ Compare spec vs audit
â”œâ”€â”€ Identify gaps
â””â”€â”€ Output: Questions for User

PHASE 0C: PRE-IMPLEMENTATION
â”œâ”€â”€ Read: GRAMMAR_COACH_PRE_IMPLEMENTATION.md
â”œâ”€â”€ User answers questions
â”œâ”€â”€ Create seed data files
â”œâ”€â”€ Commit all planning docs
â””â”€â”€ Output: Ready for Implementation â†’ PR

PHASES 1-5: IMPLEMENTATION
â”œâ”€â”€ Read: GRAMMAR_COACH_EXECUTION_GUIDE.md
â”œâ”€â”€ Phase 1: Foundation
â”œâ”€â”€ Phase 2: Core Analysis
â”œâ”€â”€ Phase 3: Enhanced Features
â”œâ”€â”€ Phase 4: UI Integration
â””â”€â”€ Phase 5: Polish
```

### Document Links

| Order | Document | Purpose |
|-------|----------|---------|
| 1 | [GRAMMAR_COACH_PHASE_0_AUDIT.md](./GRAMMAR_COACH_PHASE_0_AUDIT.md) | Scan codebase, document current state |
| 2 | [GRAMMAR_COACH_GAP_ANALYSIS.md](./GRAMMAR_COACH_GAP_ANALYSIS.md) | Compare spec vs reality, find gaps |
| 3 | [GRAMMAR_COACH_PRE_IMPLEMENTATION.md](./GRAMMAR_COACH_PRE_IMPLEMENTATION.md) | Create seed data, commit docs |
| 4 | [GRAMMAR_COACH_EXECUTION_GUIDE.md](./GRAMMAR_COACH_EXECUTION_GUIDE.md) | Implementation steps |

### Reference Documents

| Document | Purpose |
|----------|---------|
| [1.1.1.1_RULES.md](./1.1.1.1_RULES.md) | Coding standards (CRITICAL) |
| [LANGUAGE_COACH_AUDIT_stage_0.md](./LANGUAGE_COACH_AUDIT_stage_0.md) | Previous audit |
| [CLAUDE.md](./CLAUDE.md) / [GEMINI.md](./GEMINI.md) | Stack configuration |

---

## Table of Contents
1. [Philosophy](#philosophy-grammar-coach-as-lsp)
2. [Architecture Decisions](#architecture-decisions)
3. [SLOT System Design](#slot-system-design)
4. [Tag System Design](#tag-system-design)
5. [Quiz-Specific Parsing](#quiz-specific-parsing)
6. [Algorithm Phases](#algorithm-phases)
7. [API Response Structure](#api-response-structure)
8. [UI Design](#ui-design)
9. [Edge Cases](#edge-case-handling)
10. [Compliance Rules](#compliance-rules)
11. [Risk Assessment](#risk-assessment)
12. [Future Enhancements](#future-enhancements)
13. [Migration Guide](#migration-from-v20)

---

## Philosophy: Grammar Coach as LSP

This algorithm is designed like a **Language Server Protocol** for worksheet creation:
- Provides diagnostics (overuse, underuse, imbalance)
- Suggests vocabulary alternatives
- Shows word locations (like "Go to definition")
- Recommends changes (like code actions)
- **Human decides** - the teacher applies/ignores recommendations

**Not automation. Intelligent recommendations.**

---

## Version History

| Version | Approach | Problem |
|---------|----------|---------|
| v1.0 | Dynamic threshold from data | Threshold moved with overuse |
| v2.0 | Fixed threshold (3 or 15%) | No lesson awareness, no location tracking |
| v3.0 | Adaptive analysis with lesson scope | Current version |

---

## Architecture Decisions

### Confirmed Decisions

| Decision | Choice | Rationale |
|----------|--------|-----------|
| Coach UI | Unified with tabs | 1.1.1.1 compliant, single responsibility |
| StyleCoach | Mark for removal | No clear purpose, clean codebase |
| Tag system | Flat with Category + Aspects | Solves multi-classification problem |
| Slot detection | Particle-based | Simple, reliable for N5 |
| ã¾ã™å½¢ compliance | ENFORCE | Block non-compliant imports |
| Notifications | Smart dismissal | Reappear if worse, "ignore word" option |
| Analysis trigger | Debounced real-time | After meaningful threshold reached |

### UI Structure: Unified Language Coach

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LANGUAGE COACH                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  [Distribution] [Suggestions] [Patterns]  â† Tabs            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Distribution Tab:                                          â”‚
â”‚  â””â”€ Word frequency analysis (from old VocabCoach)           â”‚
â”‚                                                             â”‚
â”‚  Suggestions Tab:                                           â”‚
â”‚  â””â”€ Sentence-aware recommendations with locations           â”‚
â”‚                                                             â”‚
â”‚  Patterns Tab:                                              â”‚
â”‚  â””â”€ Grammar structure coverage (slots used, missing)        â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## SLOT System Design

### What is a SLOT?

A SLOT is a grammatical position in a Japanese sentence that:
- Accepts specific types of vocabulary (based on tags)
- Is marked by particles (ã‚’, ã«, ã§, etc.)
- Has semantic constraints based on the verb

### Schema Design

```sql
-- Core slot definitions
CREATE TABLE slot_definition (
    id BIGINT PRIMARY KEY,
    name VARCHAR(50) NOT NULL,           -- e.g., "OBJECT", "DIRECTION"
    particles TEXT NOT NULL,             -- JSON: ["ã‚’"] or ["ã«", "ã¸"]
    description VARCHAR(255),            -- Human-readable explanation
    lesson_introduced INT,               -- When this slot is first taught
    UNIQUE(name)
);

-- Default accepted tags per slot (can be overridden by verb)
CREATE TABLE slot_default_tags (
    id BIGINT PRIMARY KEY,
    slot_id BIGINT REFERENCES slot_definition(id),
    tag_name VARCHAR(50) NOT NULL,       -- e.g., "thing", "buyable"
    is_required BOOLEAN DEFAULT false    -- Must have this tag vs. may have
);

-- Verb-specific slot constraints (v4.0 enhancement)
CREATE TABLE verb_slot_constraint (
    id BIGINT PRIMARY KEY,
    verb_base_form VARCHAR(50) NOT NULL, -- e.g., "é£Ÿã¹ã‚‹"
    slot_name VARCHAR(50) NOT NULL,      -- e.g., "OBJECT"
    required_tags TEXT NOT NULL,         -- JSON: ["edible", "food"]
    excluded_tags TEXT,                  -- JSON: ["vehicle", "building"]
    lesson_id INT                        -- When this verb is introduced
);
```

### Slot Definitions for N5

| Slot Name | Particles | Default Tags | Example |
|-----------|-----------|--------------|---------|
| SUBJECT | ã¯, ãŒ | person, pronoun | ç§**ã¯**ã€å…ˆç”Ÿ**ãŒ** |
| OBJECT | ã‚’ | thing, person | æœ¬**ã‚’**ã€å‹é”**ã‚’** |
| LOCATION | ã§ | place | å­¦æ ¡**ã§**ã€é§…**ã§** |
| DIRECTION | ã«, ã¸ | place | å­¦æ ¡**ã«**ã€æ—¥æœ¬**ã¸** |
| TIME | ã« | time | 3æ™‚**ã«**ã€æœˆæ›œæ—¥**ã«** |
| INSTRUMENT | ã§ | thing, vehicle | ãƒã‚¹**ã§**ã€ãƒšãƒ³**ã§** |
| COMPANION | ã¨ | person | å‹é”**ã¨** |
| SOURCE | ã‹ã‚‰ | place, person | æ±äº¬**ã‹ã‚‰**ã€å…ˆç”Ÿ**ã‹ã‚‰** |
| GOAL | ã¾ã§ | place, time | é§…**ã¾ã§**ã€5æ™‚**ã¾ã§** |

### Slot Detection Algorithm

```
FUNCTION detectSlots(tokens: List<Token>) -> List<SlotAssignment>:
    assignments = []

    FOR i = 0 TO tokens.length - 1:
        token = tokens[i]

        IF token.pos == "åŠ©è©" (particle):
            // Look backwards for the word this particle marks
            IF i > 0:
                markedWord = tokens[i - 1]
                slot = lookupSlotByParticle(token.surface)

                IF slot != null:
                    assignments.add({
                        word: markedWord,
                        slot: slot,
                        particle: token.surface,
                        position: i - 1
                    })

    RETURN assignments
```

### Slot Analysis Output

```typescript
interface SlotAnalysis {
    slotsUsed: Map<SlotName, number>;     // OBJECT: 5, DIRECTION: 2
    slotsMissing: SlotName[];             // Slots with 0 usage
    slotDistribution: SlotDistributionItem[];
    suggestions: SlotSuggestion[];
}

interface SlotDistributionItem {
    slot: string;
    count: number;
    words: string[];
    coverage: number;  // % of available words for this slot used
}

interface SlotSuggestion {
    message: string;  // "Consider adding DIRECTION words"
    slot: string;
    suggestedWords: VocabSuggestion[];
}
```

### Risk Assessment: SLOT System

| Risk | Severity | Mitigation |
|------|----------|------------|
| Parser misidentifies particles | Medium | Log decisions, allow manual override |
| Complexity creep (verb constraints) | High | Strict v3.0 scope, defer verb rules to v4.0 |
| Performance with many checks | Low | Cache slot lookups, batch processing |
| Incomplete N5 coverage | Medium | Start with top 10 patterns, iterate |
| User confusion (grammar terms) | Medium | Use simple language in UI |

---

## Tag System Design

### The Category + Aspects Model

**Problem:** Pure hierarchy fails when items belong to multiple branches.
- è»Š (car) is a thing, a vehicle, buyable, and giftable
- Hierarchy forces single parent, but car has multiple classifications

**Solution:** Two-level flat system with Category (what it IS) and Aspects (what it CAN DO).

### Schema Design

```sql
-- Vocabulary with category and aspects
CREATE TABLE vocab (
    id BIGINT PRIMARY KEY,
    lesson_id INT NOT NULL,
    display_form VARCHAR(100) NOT NULL,  -- Original form from CSV
    base_form VARCHAR(100) NOT NULL,     -- Kuromoji-normalized
    part_of_speech VARCHAR(50),          -- From Kuromoji
    category VARCHAR(50) NOT NULL,       -- person, thing, place, time, action, descriptor
    aspects TEXT                         -- JSON array: ["buyable", "vehicle", "expensive"]
);

-- Aspect definitions for reference/validation
CREATE TABLE aspect_definition (
    id BIGINT PRIMARY KEY,
    name VARCHAR(50) UNIQUE NOT NULL,    -- e.g., "buyable"
    description VARCHAR(255),            -- "Can be purchased"
    applicable_categories TEXT           -- JSON: ["thing", "place"] - which categories can have this
);
```

### Categories (Mutually Exclusive)

| Category | Description | Examples |
|----------|-------------|----------|
| person | Human beings, roles | ç§, å…ˆç”Ÿ, å­¦ç”Ÿ, å‹é” |
| thing | Physical objects | æœ¬, è»Š, ãƒšãƒ³, é›»è©± |
| place | Locations | å­¦æ ¡, é§…, æ—¥æœ¬, éƒ¨å±‹ |
| time | Time expressions | ä»Šæ—¥, æ˜æ—¥, 3æ™‚, æœˆæ›œæ—¥ |
| action | Verbs (ã¾ã™ form) | é£Ÿã¹ã¾ã™, è¡Œãã¾ã™, è²·ã„ã¾ã™ |
| descriptor | Adjectives | é«˜ã„, ãã‚Œã„, å¥½ã |

### Aspects (Combinable)

**Capability Aspects:**
- buyable, giftable, edible, drinkable, wearable, readable, usable

**Type Aspects:**
- vehicle, tool, container, furniture, electronics, food, drink, clothing

**Context Aspects:**
- school_related, work_related, home_related, travel_related

**Property Aspects:**
- expensive, cheap, large, small, animate, inanimate

### Query Examples

```sql
-- What can I buy?
SELECT * FROM vocab WHERE aspects @> '["buyable"]';

-- What vehicles exist in Lesson 5?
SELECT * FROM vocab
WHERE lesson_id = 5 AND aspects @> '["vehicle"]';

-- What school-related things can I buy?
SELECT * FROM vocab
WHERE category = 'thing'
  AND aspects @> '["school_related", "buyable"]';
```

### Contextual Narrowing Algorithm

```
FUNCTION suggestForSlot(
    slot: SlotName,
    verb: String,
    contextWords: List<String>,
    lessonRange: Range
) -> List<VocabSuggestion>:

    // Step 1: Get base candidates from slot's default tags
    candidates = getVocabBySlotTags(slot, lessonRange)

    // Step 2: Apply verb-specific constraints (if defined)
    IF hasVerbConstraint(verb, slot):
        constraint = getVerbConstraint(verb, slot)
        candidates = filterByTags(candidates, constraint.requiredTags)
        candidates = excludeByTags(candidates, constraint.excludedTags)

    // Step 3: Apply contextual narrowing
    FOR contextWord IN contextWords:
        wordAspects = getAspects(contextWord)

        // If context contains "å­¦æ ¡", boost school_related words
        IF "school_related" IN wordAspects:
            candidates = boostByAspect(candidates, "school_related")

        // If context contains "é£Ÿã¹ã¾ã™", require edible
        IF isEatingVerb(contextWord):
            candidates = filterByAspect(candidates, "edible")

    // Step 4: Sort by lesson priority and current usage
    candidates = sortByPriority(candidates, targetLesson)

    RETURN top(candidates, 10)
```

---

## Quiz-Specific Parsing

### Item Type Detection

| Item Type | Structure | Parsing Mode |
|-----------|-----------|--------------|
| VOCAB | `{ term, meaning }` | SIMPLE - extract term only |
| GRID | `{ content }` or boxes | SIMPLE - extract content string |
| MULTIPLE_CHOICE | `{ prompt, options[] }` | STRUCTURED - parse each option |
| MATCHING | `{ pairs[] }` | STRUCTURED - parse match values |
| CLOZE | `{ passage }` | SPECIAL - detect blanks |
| REARRANGE | `{ fragments[] }` | VOCABULARY_ONLY - no structure |
| TRUE_FALSE | `{ statement }` | SKIP - prompts only |

### Rearrange Question Handling

```
Input: [ã¯ãƒ»ã‚ã’ã¾ã—ãŸãƒ»ã«ãƒ»ã‚ãŸã—ãƒ»ã‚’ãƒ»éˆ´æœ¨ã•ã‚“ãƒ»è»Šãƒ»]

PARSING MODE: VOCABULARY_ONLY

Analysis:
â”œâ”€ Tokens extracted: [ã¯, ã‚ã’ã¾ã—ãŸ, ã«, ã‚ãŸã—, ã‚’, éˆ´æœ¨ã•ã‚“, è»Š]
â”œâ”€ Particles found: [ã¯, ã«, ã‚’]
â”œâ”€ Content words: [ã‚ãŸã—, éˆ´æœ¨ã•ã‚“, è»Š, ã‚ã’ã¾ã—ãŸ]
â”œâ”€ Structure: CANNOT_DETERMINE (scrambled)
â””â”€ Note: "Rearrange question - vocabulary analyzed, structure skipped"

Output:
â”œâ”€ Word frequency counted âœ“
â”œâ”€ Vocabulary coverage checked âœ“
â”œâ”€ Slot analysis: SKIPPED
â””â”€ Grammar pattern: SKIPPED
```

### Cloze Question Handling

```
Input: ã‚ãŸã—ã¯ï¼¿ï¼¿ï¼¿ã‚’ãŸã¹ã¾ã™

PARSING MODE: CLOZE_AWARE

Step 1: Detect blank
â”œâ”€ Blank patterns: ï¼¿ï¼¿ï¼¿, ___, ã€ã€€ã€‘, (   ), ___
â”œâ”€ Found: ï¼¿ï¼¿ï¼¿ at position 4

Step 2: Analyze surrounding context
â”œâ”€ Before blank: ã‚ãŸã—ã¯
â”œâ”€ After blank: ã‚’ãŸã¹ã¾ã™
â”œâ”€ Particle after blank: ã‚’
â”œâ”€ Verb: é£Ÿã¹ã¾ã™ (to eat)

Step 3: Infer expected slot
â”œâ”€ Particle ã‚’ â†’ OBJECT slot
â”œâ”€ Verb é£Ÿã¹ã‚‹ â†’ requires [edible, food]
â”œâ”€ Expected answer tags: [edible, food, thing]

Step 4: Generate suggestions
â”œâ”€ From lesson vocab with [edible] aspect:
â”‚   â”œâ”€ ã”ã¯ã‚“ (rice)
â”‚   â”œâ”€ ãƒ‘ãƒ³ (bread)
â”‚   â”œâ”€ ã‚Šã‚“ã” (apple)
â”‚   â””â”€ ã•ã‹ãª (fish)
â””â”€ Can validate if answer key matches expected slot âœ“
```

### Blank Detection Regex

```java
// Patterns to detect blanks in cloze questions
private static final Pattern BLANK_PATTERNS = Pattern.compile(
    "ï¼¿{2,}|_{2,}|ã€\\s*ã€‘|ï¼ˆ\\s*ï¼‰|\\(\\s*\\)|\\[\\s*\\]"
);

// Extract context around blank
public BlankContext analyzeBlank(String passage) {
    Matcher m = BLANK_PATTERNS.matcher(passage);
    if (m.find()) {
        int blankStart = m.start();
        int blankEnd = m.end();

        String before = passage.substring(0, blankStart);
        String after = passage.substring(blankEnd);

        // Find particle immediately after blank
        String particleAfter = extractFirstParticle(after);

        return new BlankContext(blankStart, particleAfter, before, after);
    }
    return null;
}
```

---

## Compliance Rules

### ã¾ã™å½¢ Enforcement

**Policy:** ENFORCE - Block non-compliant imports with clear error message.

```
CSV Import Validation:

VALID:
â”œâ”€ é£Ÿã¹ã¾ã™ âœ“ (ã¾ã™ form)
â”œâ”€ è¡Œãã¾ã™ âœ“ (ã¾ã™ form)
â”œâ”€ é«˜ã„ âœ“ (ã„-adjective, no change needed)
â”œâ”€ ãã‚Œã„ âœ“ (ãª-adjective, ãª removed)
â””â”€ é™ã‹ âœ“ (ãª-adjective, ãª removed)

INVALID:
â”œâ”€ é£Ÿã¹ã‚‹ âœ— â†’ ERROR: "Dictionary form detected. Expected: é£Ÿã¹ã¾ã™"
â”œâ”€ é£Ÿã¹ã¦ âœ— â†’ ERROR: "ã¦-form detected. Expected: é£Ÿã¹ã¾ã™"
â”œâ”€ ãã‚Œã„ãª âœ— â†’ ERROR: "ãª-adjective should not include ãª. Expected: ãã‚Œã„"
â””â”€ è¡Œã£ãŸ âœ— â†’ ERROR: "Past tense detected. Expected: è¡Œãã¾ã™"
```

### Compliance Check Algorithm

```java
public ValidationResult validateVocabEntry(String word, String pos) {
    // Check verb forms
    if (pos.startsWith("å‹•è©")) {
        if (!word.endsWith("ã¾ã™")) {
            String suggestion = convertToMasuForm(word);
            return ValidationResult.error(
                "Verb must be in ã¾ã™ form. Found: " + word +
                ". Expected: " + suggestion
            );
        }
    }

    // Check ãª-adjective
    if (pos.contains("å½¢å®¹å‹•è©") && word.endsWith("ãª")) {
        String corrected = word.substring(0, word.length() - 1);
        return ValidationResult.error(
            "ãª-adjective should not include ãª. Found: " + word +
            ". Expected: " + corrected
        );
    }

    return ValidationResult.valid();
}
```

### Compliance Report UI

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  IMPORT COMPLIANCE REPORT                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  File: lesson5_vocab.csv                                    â”‚
â”‚  Total entries: 45                                          â”‚
â”‚  Valid: 42                                                  â”‚
â”‚  Errors: 3                                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ”´ Line 12: é£Ÿã¹ã‚‹                                          â”‚
â”‚     Error: Dictionary form detected                         â”‚
â”‚     Expected: é£Ÿã¹ã¾ã™                                       â”‚
â”‚     [Auto-fix] [Skip] [Edit manually]                       â”‚
â”‚                                                             â”‚
â”‚  ğŸ”´ Line 28: ãã‚Œã„ãª                                        â”‚
â”‚     Error: ãª-adjective should not include ãª               â”‚
â”‚     Expected: ãã‚Œã„                                         â”‚
â”‚     [Auto-fix] [Skip] [Edit manually]                       â”‚
â”‚                                                             â”‚
â”‚  ğŸ”´ Line 33: è¡Œã£ãŸ                                          â”‚
â”‚     Error: Past tense detected                              â”‚
â”‚     Expected: è¡Œãã¾ã™                                       â”‚
â”‚     [Auto-fix] [Skip] [Edit manually]                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  [Fix All] [Import Valid Only] [Cancel]                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Notification Watcher System

### Behavior Specification

```
NOTIFICATION LIFECYCLE:

1. CREATED
   â””â”€ Diagnostic detected, notification appears in sidebar

2. WATCHING
   â””â”€ System observes the flagged item for changes

3. RESOLVED (auto)
   â””â”€ User fixes the issue â†’ notification self-removes

4. ESCALATED
   â””â”€ Issue gets worse â†’ severity increases, reappears if dismissed

5. DISMISSED (manual)
   â””â”€ User clicks dismiss â†’ hidden but still watching

6. IGNORED (permanent)
   â””â”€ User clicks "Don't show for this word" â†’ never show again for this word
```

### Watcher Logic

```typescript
interface NotificationWatcher {
    id: string;
    targetItemIndex: number;
    targetWord: string;
    originalSeverity: Severity;
    currentSeverity: Severity;
    status: 'active' | 'dismissed' | 'ignored' | 'resolved';
    threshold: number;  // At time of creation
    currentCount: number;
}

function evaluateWatcher(watcher: NotificationWatcher, newAnalysis: AnalysisResult): WatcherAction {
    const newCount = newAnalysis.wordCounts[watcher.targetWord] || 0;
    const newThreshold = newAnalysis.distribution.overuseThreshold;

    // Check if resolved
    if (newCount <= newThreshold) {
        return { action: 'RESOLVE', reason: 'Issue fixed' };
    }

    // Check if escalated
    if (newCount > watcher.currentCount) {
        if (watcher.status === 'dismissed') {
            return { action: 'REAPPEAR', reason: 'Issue worsened' };
        }
        return { action: 'ESCALATE', newSeverity: calculateSeverity(newCount, newThreshold) };
    }

    // Check if threshold changed (worksheet grew)
    if (newThreshold > watcher.threshold) {
        const newSeverity = calculateSeverity(newCount, newThreshold);
        if (newSeverity < watcher.currentSeverity) {
            return { action: 'DOWNGRADE', newSeverity };
        }
    }

    return { action: 'NO_CHANGE' };
}
```

### UI Notification Component

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ”´ å­¦ç”Ÿ overused (12x, threshold: 4)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Locations: Item 2, Item 5, Item 7, +9 more                 â”‚
â”‚                                                             â”‚
â”‚  Suggestions:                                               â”‚
â”‚  â””â”€ ç”Ÿå¾’ (0 uses), å…ˆç”Ÿ (1 use), äºº (0 uses)                â”‚
â”‚                                                             â”‚
â”‚  [Dismiss] [Ignore "å­¦ç”Ÿ" permanently]                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

After user clicks [Dismiss]:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  (Watching: å­¦ç”Ÿ - will reappear if count increases)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

If count increases from 12 to 15:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ”´ å­¦ç”Ÿ overused (15x â†‘3, threshold: 4)                     â”‚
â”‚  âš ï¸ Issue worsened since dismissal                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ...                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Risk Assessment

### Implementation Risks

| Risk | Severity | Likelihood | Mitigation |
|------|----------|------------|------------|
| Kuromoji parser edge cases | Medium | Medium | Extensive testing with real Minna no Nihongo content |
| Slot detection accuracy | Medium | Low | Particle-based detection is reliable; log ambiguous cases |
| Performance with real-time analysis | Low | Low | Debounce, batch processing, max 10 pages |
| Complexity creep | High | Medium | Strict v3.0 scope boundary; defer verb constraints |
| Tag maintenance burden | Medium | Medium | Start with minimal aspects, add as needed |
| UI confusion | Medium | Medium | Simple language, tooltips, progressive disclosure |

### Scope Boundaries

**IN SCOPE for v3.0:**
- Particle-based slot detection (default mappings: ã§â†’LOCATION, ã«â†’DIRECTION)
- Category + Aspects tag system
- Cloze blank analysis
- Unified Language Coach UI (tabs: Distribution, Suggestions, Patterns)
- ã¾ã™å½¢ enforcement
- Lesson scope filtering (with fallback to all lessons)
- Location tracking for diagnostics
- Score calculation with weighted formula

**OUT OF SCOPE (v4.0+):**
- REARRANGE item type handling
- Notification watcher system
- Verb-specific slot constraints
- Semantic similarity suggestions
- Grammar point tracking entity
- Adjective metadata (ã„ vs ãª, emotion patterns)
- Valency-aware analysis
- ãŒ particle disambiguation (subject vs object of emotion)
- Full particle disambiguation (ã§, ã« context-aware)

---

## Algorithm Phases

### Phase 1: Pool Analysis

Load vocabulary for the specified lesson range and calculate pool metrics.

```
INPUT:
â”œâ”€ lesson_scope: { mode: "single" | "range", target: number, range?: [start, end] }
â””â”€ vocab_database: all vocabulary entries

OUTPUT:
â”œâ”€ pool_size: total words in scope
â”œâ”€ pool_by_category: Map<VocabTag, Word[]>
â”œâ”€ pool_by_pos: Map<PartOfSpeech, Word[]>
â”œâ”€ pool_by_lesson: Map<LessonId, Word[]>
â””â”€ lesson_priorities: Map<LessonId, "HIGH" | "LOW">
```

**Lesson Priority Rules:**
- Target lesson = HIGH priority
- All other lessons in range = LOW priority (ordered by recency: 4 â†’ 3 â†’ 2 â†’ 1)

### Phase 2: Worksheet Analysis

Extract and tokenize Japanese content, tracking exact locations.

```
INPUT:
â”œâ”€ worksheet_json: raw worksheet content
â””â”€ tokenizer: Kuromoji (via SudachiTokenizerService)

OUTPUT:
â”œâ”€ word_occurrences: Map<BaseForm, WordOccurrence>
â”‚   â””â”€ WordOccurrence: { count, locations[], category, pos, lesson }
â””â”€ sentence_boundaries: detected via ã€‚delimiter
```

**Location Tracking:**
```typescript
interface WordLocation {
  itemIndex: number;        // Which item in worksheet
  itemType: string;         // GRID, VOCAB, MULTIPLE_CHOICE, etc.
  sentenceIndex?: number;   // Which sentence within item
  charStart?: number;       // Character position for highlighting
  charEnd?: number;
  preview: string;          // Context snippet (e.g., "...ç§ã¯å­¦ç”Ÿã§ã™...")
}
```

### Phase 3: Validity Calculation

Determine statistical reliability based on lesson content, not fixed numbers.

```
METRICS:
â”œâ”€ overall_coverage = unique_words_used / pool_size
â”œâ”€ category_coverage[] = for each VocabTag:
â”‚     words_used_with_tag / words_in_pool_with_tag
â”œâ”€ min_category_coverage = min(category_coverage[])
â”œâ”€ categories_touched = count(coverage > 0)
â””â”€ total_categories = count(distinct tags in pool)

VALIDITY RULES:
â”œâ”€ HIGH:   overall >= 60% AND min_category >= 30% AND categories_touched >= 80%
â”œâ”€ MEDIUM: overall >= 30% AND categories_touched >= 60%
â””â”€ LOW:    everything else

OUTPUT:
â”œâ”€ validity: "HIGH" | "MEDIUM" | "LOW"
â””â”€ validity_note: contextual explanation
```

**Example Validity Notes:**
```
LOW: "Worksheet covers 15% of vocabulary. Missing categories: places, animals.
     Statistics are indicative only."

MEDIUM: "Good category spread (83%). Places underrepresented (25%).
        Consider adding å­¦æ ¡, é§…, or ç—…é™¢."

HIGH: "Excellent coverage (72%) across all categories.
      Distribution statistics are highly reliable."
```

### Phase 4: Distribution Analysis

Calculate expected distribution and identify outliers.

```
METRICS:
â”œâ”€ total_word_count = sum of all occurrences (with repetition)
â”œâ”€ unique_word_count = distinct words used
â”œâ”€ expected_per_word = total_word_count / unique_word_count
â”œâ”€ std_deviation = stddev(word_counts)
â”œâ”€ overuse_threshold = expected + (2 Ã— std_deviation)
â””â”€ underuse_threshold = 0 (or < expected - std_deviation)

CATEGORY-LEVEL ANALYSIS:
For each VocabTag category:
â”œâ”€ category_pool_size = words in pool with this tag
â”œâ”€ category_used = words in worksheet with this tag
â”œâ”€ category_coverage = category_used / category_pool_size
â””â”€ Flag if category_coverage < 30% but other categories > 60%
```

### Phase 5: Suggestion Generation

Generate replacement suggestions with priority ordering.

```
FOR OVERUSED WORDS:
â”œâ”€ Find alternatives from same category (VocabTag)
â”œâ”€ Sort by:
â”‚   1. Lesson priority (HIGH before LOW)
â”‚   2. Current usage (0 uses first)
â”‚   3. POS match (same POS first)
â”œâ”€ PRIMARY suggestions: same POS
â”œâ”€ SECONDARY suggestions: different POS (requires sentence restructure)
â””â”€ Limit: Top 5 shown, expandable to all

FOR UNDERUSED WORDS:
â”œâ”€ Identify words with 0 uses from target lesson
â”œâ”€ Group by category
â””â”€ Recommend for inclusion
```

**Suggestion Structure:**
```typescript
interface ReplacementSuggestion {
  word: string;
  currentUsage: number;
  lesson: number;
  lessonPriority: "HIGH" | "LOW";
  pos: string;
  isSamePOS: boolean;
  category: string;
}
```

### Phase 6: Diagnostic Generation

Generate LSP-style diagnostics with severity levels.

```
SEVERITY LEVELS:
â”œâ”€ ERROR (red):   Critical imbalance - word used 3Ïƒ+ above expected
â”œâ”€ WARNING (yellow): Notable overuse - word used 2Ïƒ above expected
â”œâ”€ INFO (blue):   Suggestion - underused words available
â””â”€ HINT (gray):   Context - validity notes, missing categories

EACH DIAGNOSTIC INCLUDES:
â”œâ”€ severity
â”œâ”€ message
â”œâ”€ targetWord (if applicable)
â”œâ”€ actualCount vs expectedCount
â”œâ”€ locations[] (clickable)
â”œâ”€ suggestions[] (primary + secondary)
â””â”€ category context
```

---

## API Response Structure

```typescript
interface GrammarCoachAnalysisResult {
  // Metadata
  meta: {
    worksheetWordCount: number;      // Total with repetition
    uniqueWordsUsed: number;         // Distinct words
    vocabPoolSize: number;           // Available in lesson scope
    lessonScope: {
      mode: "single" | "range";
      target: number;
      range?: [number, number];
    };
    validity: "HIGH" | "MEDIUM" | "LOW";
    validityNote: string;
  };

  // Distribution overview
  distribution: {
    expectedPerWord: number;
    stdDeviation: number;
    overuseThreshold: number;
    categoryBreakdown: Array<{
      category: string;
      poolSize: number;
      used: number;
      coverage: number;  // percentage
    }>;
  };

  // LSP-style diagnostics
  diagnostics: Array<{
    severity: "ERROR" | "WARNING" | "INFO" | "HINT";
    type: "OVERUSE" | "UNDERUSE" | "CATEGORY_IMBALANCE" | "VALIDITY_NOTE";
    message: string;
    word?: string;
    actualCount?: number;
    expectedCount?: number;
    locations?: WordLocation[];
    suggestions?: {
      primary: ReplacementSuggestion[];    // Same POS
      secondary: ReplacementSuggestion[];  // Different POS
    };
  }>;

  // Slot analysis (grammar structure)
  slotAnalysis: {
    slotsUsed: Record<string, number>;  // { "OBJECT": 5, "DIRECTION": 2 }
    slotsMissing: string[];             // ["TIME", "COMPANION"]
    slotDistribution: Array<{
      slot: string;
      count: number;
      words: string[];
      coverage: number;
    }>;
    suggestions: Array<{
      message: string;
      slot: string;
      suggestedWords: string[];
    }>;
    // Human-readable summary
    summary: string;  // "Your worksheet asks WHO (5x) and WHAT (3x) but never asks WHERE or WHEN."
  };

  // Cloze analysis (if applicable)
  clozeAnalysis?: Array<{
    itemIndex: number;
    blankPosition: number;
    inferredSlot: string;
    expectedTags: string[];
    suggestedAnswers: string[];
  }>;

  // Summary score
  score: {
    value: number;           // 0-100
    interpretation: string;  // "Well balanced" | "Needs attention" | etc.
  };
}
```

---

## Filtering Rules (Unchanged from v2.0)

### What Gets Counted
- Japanese text only (Hiragana, Katakana, Kanji regex)
- Words that exist in vocab database for the lesson scope
- Inner item content only:
  - VOCAB: `term` field
  - GRID: `box.value` fields
  - MULTIPLE_CHOICE: `option.text` fields
  - MATCHING: `match` field
  - CLOZE: `passage` field
  - CARD: `content` field

### What Gets Ignored
- Prompts (Vietnamese/English)
- Meanings (Vietnamese translations)
- Headers and metadata
- TRUE_FALSE items (prompts only)
- Non-Japanese text

---

## Frontend Integration

### Clickable Locations
Each diagnostic's locations are clickable:
1. User clicks location in panel
2. Worksheet scrolls to that item
3. Word is highlighted with pulse animation

```typescript
function scrollToItem(itemIndex: number, word: string) {
  const element = document.querySelector(`[data-item-index="${itemIndex}"]`);
  element.scrollIntoView({ behavior: 'smooth', block: 'center' });
  highlightWord(element, word);
}
```

### Suggestion Display
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ”´ ERROR: å­¦ç”Ÿ overused (12x, expected ~4x)                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Locations (click to jump):                                 â”‚
â”‚  â”œâ”€ [Item 2: GRID] å­¦ç”Ÿã®åå‰ã¯...                          â”‚
â”‚  â”œâ”€ [Item 5: VOCAB] å­¦ç”Ÿ                                    â”‚
â”‚  â””â”€ ... 10 more                                             â”‚
â”‚                                                             â”‚
â”‚  PRIMARY (same POS: åè©):                                   â”‚
â”‚  â”œâ”€ ç”Ÿå¾’ (0 uses, L5) â˜… HIGH priority                       â”‚
â”‚  â”œâ”€ å…ˆç”Ÿ (1 use, L5) â˜… HIGH priority                        â”‚
â”‚  â””â”€ äºº (0 uses, L4)                                         â”‚
â”‚                                                             â”‚
â”‚  SECONDARY (different POS):                                 â”‚
â”‚  â””â”€ å‹‰å¼·ã™ã‚‹ (0 uses, L5, å‹•è©) - requires restructure      â”‚
â”‚                                                             â”‚
â”‚  [â–¼ Show 12 more alternatives...]                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Edge Case Handling

### No Vocabulary for Target Lesson
Return structured response with `validity: "INVALID"` and descriptive note.
UI should display: "No vocabulary found for Lesson X. Import lesson data first."

### Unknown Words (Out of Scope)
Flag as `WARNING` with type `OUT_OF_SCOPE`.
Message: "Word not in lesson vocabulary. May be out of scope or needs database update."
Include locations so user can review.

### All Words Used Exactly Once
Context-dependent response based on coverage:

```
IF coverage >= 70%:
  â†’ INFO: "Excellent variety! Each word appears once.
           Good for exposure. Consider repetition if drilling needed."

ELIF coverage >= 40%:
  â†’ INFO: "Good variety. Each word unique.
           Consider adding more words for comprehensive coverage."

ELSE:
  â†’ HINT: "Limited sample. Each word appears once.
           Expand worksheet for meaningful distribution analysis."
```

### Kuromoji Edge Cases
| Case | Handling |
|------|----------|
| Unknown words | Flag as UNRECOGNIZED, skip from analysis |
| Compound splitting | Check vocab DB first for compound match |
| Particle attachment | Post-process to strip common particles |
| Katakana loanwords | Accept surface form as base form |

---

## Score Calculation (Option A: Weighted Formula)

```
score = 100
      - (errors Ã— 15)
      - (warnings Ã— 5)
      - (imbalanced_categories Ã— 10)
      - (validity_penalty)  // LOW: -10, MEDIUM: -5

score = max(0, score)
```

**Interpretation:**
| Score | Label | Meaning |
|-------|-------|---------|
| 90-100 | Excellent | Well-balanced, comprehensive |
| 70-89 | Good | Minor issues, generally solid |
| 50-69 | Needs Work | Notable imbalances to address |
| 0-49 | Review Required | Significant distribution problems |

---

## Auto-Tagging Vision (Future Enhancement)

### Phase 1: Pattern-Based Heuristics
```
IF word ends with äºº/è€…/å“¡/å®¶ â†’ suggest "person"
IF word ends with åº—/å±‹/é¤¨/æ‰€ â†’ suggest "place"
IF word ends with ç‰©/å“/å…· â†’ suggest "thing"
IF POS = å‹•è©-è‡ªç«‹ â†’ suggest "action"
IF POS = å½¢å®¹è© â†’ suggest "descriptor"
```

### Phase 2: Co-occurrence Learning
Track tag patterns within lessons. Suggest tags based on:
- POS similarity
- Lesson context
- Co-occurring words

### Key Principle
Never auto-apply. Always **suggest** and let human confirm.

---

## Future Enhancements

### Grammar Point Tracking (Not Yet Implemented)
```
GrammarPoint entity:
â”œâ”€ lessonId
â”œâ”€ name: "negative form"
â”œâ”€ pattern: "ã€œã¾ã›ã‚“" or "ã€œãªã„"
â”œâ”€ category: VERB_CONJUGATION | PARTICLE | SENTENCE_END

Analysis would include:
â”œâ”€ "Worksheet uses positive forms 15x, negative 2x"
â”œâ”€ "Question pattern (ã‹) not used - consider adding"
â””â”€ Grammar coverage affects validity calculation
```

### Semantic Similarity
- Suggest synonyms based on meaning, not just category
- Requires additional data source or embedding model

### Valency and Word Metadata (v4.0 Vision)

**The Problem:** Some words have special grammatical requirements.

**Adjective Types in Japanese:**

| Type | Modification | Predicate | Example |
|------|--------------|-----------|---------|
| ã„-adjective | é«˜ã„ + noun | é«˜ã„ã§ã™ | é«˜ã„ã²ã¨ (tall person) |
| ãª-adjective | å¥½ã**ãª** + noun | å¥½ãã§ã™ | å¥½ããªã²ã¨ (liked person) |

Both é«˜ã„ã²ã¨ and å¥½ããªã²ã¨ are grammatically complete. The difference is:
1. **Conjugation pattern** (ã„ vs ãª)
2. **Semantic role** (state description vs preference/emotion)

**The ãŒ Particle Ambiguity:**

ãŒ can mark different roles depending on predicate type:

```
SUBJECT marker (with action/state verbs):
  å…ˆç”ŸãŒæ¥ã¾ã™ = The teacher comes (å…ˆç”Ÿ = subject)
  æœ¬ãŒé«˜ã„ã§ã™ = The book is expensive (æœ¬ = subject)

OBJECT marker (with emotion/ability predicates):
  ã‚±ãƒ¼ã‚­ãŒå¥½ãã§ã™ = (I) like cake (ã‚±ãƒ¼ã‚­ = object of liking)
  æ—¥æœ¬èªãŒã‚ã‹ã‚Šã¾ã™ = (I) understand Japanese (æ—¥æœ¬èª = object)
  ãƒ”ã‚¢ãƒãŒã§ãã¾ã™ = (I) can play piano (ãƒ”ã‚¢ãƒ = object of ability)
```

**Proposed Metadata for v4.0:**

```typescript
interface WordMetadata {
  // Current (v3.0)
  category: Category;
  aspects: string[];
  lessonId: number;

  // Future (v4.0)
  adjectiveType?: 'ã„' | 'ãª';
  predicateType?: 'state' | 'action' | 'emotion' | 'ability';
  gaParticleRole?: 'subject' | 'object';  // What ãŒ means with this word
  requiredSlots?: string[];  // For ditransitive verbs like ã‚ã’ã¾ã™
}
```

**Emotion/Ability Predicates (ãŒ = object):**
- å¥½ãã€å«Œã„ã€ä¸Šæ‰‹ã€ä¸‹æ‰‹ã€å¾—æ„ã€è‹¦æ‰‹ (preferences/skills)
- ã‚ã‹ã‚‹ã€ã§ãã‚‹ã€è¦‹ãˆã‚‹ã€èã“ãˆã‚‹ (abilities/perceptions)
- ã»ã—ã„ã€ã€œãŸã„ (desires)

**For v3.0:** We note this complexity exists but treat ãŒ uniformly. Full disambiguation requires predicate-aware analysis in v4.0.

### Human-Readable Slot Summaries

Instead of raw slot counts, provide natural language feedback:

```
Raw data:
  SUBJECT: 5, OBJECT: 3, DIRECTION: 0, TIME: 0, LOCATION: 2

Human-readable:
  "Your worksheet asks WHO (5x) and WHAT (3x) but never asks
   WHERE someone is going or WHEN something happens.
   Consider adding direction or time expressions."
```

**Mapping:**
| Slot | Human Term | Question Word |
|------|------------|---------------|
| SUBJECT | WHO | ã ã‚Œ |
| OBJECT | WHAT | ãªã« |
| DIRECTION | WHERE (to) | ã©ã“ã¸ |
| LOCATION | WHERE (at) | ã©ã“ã§ |
| TIME | WHEN | ã„ã¤ |
| INSTRUMENT | HOW / WITH WHAT | ãªã«ã§ |
| COMPANION | WITH WHOM | ã ã‚Œã¨ |
| SOURCE | FROM WHERE | ã©ã“ã‹ã‚‰ |

---

## Implementation Priority

### Phase 1: Foundation (Must Have)
1. **Update Vocab schema** - Add `category` and `aspects` columns
2. **Create SlotDefinition table** - Store N5 slot definitions
3. **Update WorksheetScannerService** - Return locations with extracted text
4. **Particle detection** - Identify slots from particles
5. **Lesson scope filtering** - Filter vocab by lesson range

### Phase 2: Core Analysis (Must Have)
1. **Distribution analysis** - Word frequency with dynamic thresholds
2. **Category coverage** - Track usage by category/aspects
3. **Validity calculation** - Dynamic validity based on coverage
4. **Diagnostic generation** - ERROR/WARNING/INFO/HINT with locations
5. **Score calculation** - Weighted formula

### Phase 3: Enhanced Features (Should Have)
1. **Cloze blank detection** - Infer expected slot from context
2. **Rearrange handling** - Vocabulary-only mode
3. **Primary/Secondary suggestions** - Same POS vs different POS
4. **Notification watcher** - Smart dismissal with escalation

### Phase 4: UI Integration (Should Have)
1. **Unified Language Coach panel** - Three tabs (Distribution, Suggestions, Patterns)
2. **Clickable locations** - Scroll to item and highlight
3. **Suggestion display** - Show top 5 with expand
4. **Compliance report** - ã¾ã™å½¢ enforcement UI

### Phase 5: Polish (Nice to Have)
1. **Auto-tagging suggestions** - Pattern-based hints
2. **Contextual narrowing** - Boost relevant aspects based on context
3. **StyleCoach removal** - Clean up deprecated code
4. **Performance optimization** - Caching, debounce tuning
5. **Human-readable summaries** - "Your worksheet asks WHO (5x) but never WHERE"

### Estimated Complexity

| Component | Complexity | Dependencies |
|-----------|------------|--------------|
| Vocab schema update | Low | None |
| Slot definitions | Low | Vocab schema |
| Scanner with locations | Medium | None |
| Particle detection | Medium | Scanner |
| Distribution analysis | Medium | Scanner, Vocab |
| Validity calculation | Low | Distribution |
| Diagnostic generation | Medium | All analysis |
| Cloze detection | Medium | Particle detection |
| Notification watcher | High | Frontend state management |
| Unified UI | Medium | API complete |

---

## Migration from v2.0

### Breaking Changes
- Response structure completely redesigned
- `threshold` field removed (now calculated dynamically)
- `violations` renamed to `diagnostics` with richer structure

### New Required Parameters
- `lessonScope` must be provided for analysis
- Frontend must handle new diagnostic structure

### Backward Compatibility
- v2.0 endpoint can be maintained separately if needed
- Or provide adapter layer to transform v3.0 response to v2.0 format
